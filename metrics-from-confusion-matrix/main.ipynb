{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: DECISION-TREE\n",
      "Overall Precision: 0.591594\n",
      "Overall Recall: 0.600000\n",
      "Overall F1 Score: 0.595623\n",
      "Overall Accuracy: 0.974564\n",
      "Overall FAR: 0.008406\n",
      "Training Time:  37.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jlega\\.conda\\envs\\tf_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Sample confusion matrix from your previous messages\n",
    "confusion = np.array([\n",
    "    [66, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 68461, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 66337, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 106, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 43079, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 72316, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 34, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1253, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 5139, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    "])\n",
    "\n",
    "# Generate true labels\n",
    "y_true = np.repeat(range(confusion.shape[0]), np.sum(confusion, axis=1))\n",
    "# Generate predicted labels\n",
    "y_pred = np.concatenate([np.repeat(i, confusion[:, i].sum()) for i in range(confusion.shape[1])])\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_precision = precision_score(y_true, y_pred, average='macro')\n",
    "overall_recall = recall_score(y_true, y_pred, average='macro')\n",
    "overall_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "# FAR calculation for overall is not typical, provided here for completeness\n",
    "far = np.mean([(confusion[:, i].sum() - confusion[i, i]) / (np.sum(confusion, axis=0)[i] if np.sum(confusion, axis=0)[i] != 0 else 1) for i in range(confusion.shape[0])])\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"MODEL: DECISION-TREE\")\n",
    "print(\"Overall Precision: {:.6f}\".format(overall_precision))\n",
    "print(\"Overall Recall: {:.6f}\".format(overall_recall))\n",
    "print(\"Overall F1 Score: {:.6f}\".format(overall_f1))\n",
    "print(\"Overall Accuracy: {:.6f}\".format(overall_accuracy))\n",
    "print(\"Overall FAR: {:.6f}\".format(far))\n",
    "print(\"Overall AUC-ROC: 0.96\")\n",
    "print(\"Training Time: \",37.61)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: LIGHT-GBM\n",
      "Overall Precision: 0.999759\n",
      "Overall Recall: 0.999759\n",
      "Overall F1 Score: 0.999759\n",
      "Overall Accuracy: 0.999759\n",
      "Overall FAR: 0.174556\n",
      "Training Time:  77.61\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Sample confusion matrix from your previous messages\n",
    "confusion = np.array([\n",
    "[58,    3,    1,    2,    0,    0, 0,    1,    1,    0],\n",
    "    [18, 68437,    0,    3,    0,    0,    3,    0,    0, 0],\n",
    "    [0,     0, 66337,    0,    0,    0,    0,    0,    0, 0],\n",
    "    [1,     3,    0,   92,    0,    1,  0,  7,    1,    1],\n",
    "    [3,     0,    0,    1, 43073,  0,  0,    1,    1,    0],\n",
    "    [0,     0,    0,    0,    0, 72314,2,    0,    0,    0],\n",
    "    [2,     0,    0,    0,    0, 0,   24,    4,    4,    0],\n",
    "    [1,     0,    0,    0,    0, 0,   3, 1248,    1,    0],\n",
    "    [0,     0,    0,    1, 0, 0,    4,    5, 5129,    0],\n",
    "    [3,     0,    3,    0,    0,    0,    0, 0,    0,0]\n",
    "])\n",
    "\n",
    "# Generate true labels\n",
    "y_true = np.repeat(range(confusion.shape[0]), np.sum(confusion, axis=1))\n",
    "# Generate predicted labels\n",
    "y_pred = np.concatenate([np.repeat(i, confusion[:, i].sum()) for i in range(confusion.shape[1])])\n",
    "\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_precision = precision_score(y_true, y_pred, average='micro')\n",
    "overall_recall = recall_score(y_true, y_pred, average='micro')\n",
    "overall_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "# FAR calculation for overall is not typical, provided here for completeness\n",
    "far = np.mean([(confusion[:, i].sum() - confusion[i, i]) / (np.sum(confusion, axis=0)[i] if np.sum(confusion, axis=0)[i] != 0 else 1) for i in range(confusion.shape[0])])\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"MODEL: LIGHT-GBM\")\n",
    "print(\"Overall Precision: {:.6f}\".format(overall_precision))\n",
    "print(\"Overall Recall: {:.6f}\".format(overall_recall))\n",
    "print(\"Overall F1 Score: {:.6f}\".format(overall_f1))\n",
    "print(\"Overall Accuracy: {:.6f}\".format(overall_accuracy))\n",
    "print(\"Overall FAR: {:.6f}\".format(far))\n",
    "print(\"Overall AUC-ROC: 0.85\")\n",
    "print(\"Training Time: \",77.61)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: NEURAL-NETWORK\n",
      "Overall Precision: 0.994961\n",
      "Overall Recall: 0.994961\n",
      "Overall F1 Score: 0.994961\n",
      "Overall Accuracy: 0.994961\n",
      "Overall FAR: 0.019957\n",
      "Training Time:  1144.32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Sample confusion matrix from your previous messages\n",
    "confusion = np.array([\n",
    "    [66, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 68461, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 66337, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 106, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 43079, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 72316, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 7, 0, 0, 0, 27, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1253, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 5139, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    "])\n",
    "\n",
    "# Generate true labels\n",
    "y_true = np.repeat(range(confusion.shape[0]), np.sum(confusion, axis=1))\n",
    "# Generate predicted labels\n",
    "y_pred = np.concatenate([np.repeat(i, confusion[:, i].sum()) for i in range(confusion.shape[1])])\n",
    "\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_precision = precision_score(y_true, y_pred, average='micro')\n",
    "overall_recall = recall_score(y_true, y_pred, average='micro')\n",
    "overall_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "# FAR calculation for overall is not typical, provided here for completeness\n",
    "far = np.mean([(confusion[:, i].sum() - confusion[i, i]) / (np.sum(confusion, axis=0)[i] if np.sum(confusion, axis=0)[i] != 0 else 1) for i in range(confusion.shape[0])])\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"MODEL: NEURAL-NETWORK\")\n",
    "print(\"Overall Precision: {:.6f}\".format(overall_precision))\n",
    "print(\"Overall Recall: {:.6f}\".format(overall_recall))\n",
    "print(\"Overall F1 Score: {:.6f}\".format(overall_f1))\n",
    "print(\"Overall Accuracy: {:.6f}\".format(overall_accuracy))\n",
    "print(\"Overall FAR: {:.6f}\".format(far))\n",
    "print(\"Overall AUC-ROC: 0.99\")\n",
    "print(\"Training Time: \",1144.32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: XGBoost\n",
      "Overall Precision: 1.000000\n",
      "Overall Recall: 1.000000\n",
      "Overall F1 Score: 1.000000\n",
      "Overall Accuracy: 1.000000\n",
      "Overall FAR: 0.000000\n",
      "Training Time:  545.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Sample confusion matrix from your previous messages\n",
    "confusion = np.array([\n",
    "    [66, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 68461, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 66337, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 106, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 43079, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 72316, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 34, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1253, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 5139, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    "])\n",
    "\n",
    "# Generate true labels\n",
    "y_true = np.repeat(range(confusion.shape[0]), np.sum(confusion, axis=1))\n",
    "# Generate predicted labels\n",
    "y_pred = np.concatenate([np.repeat(i, confusion[:, i].sum()) for i in range(confusion.shape[1])])\n",
    "\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_precision = precision_score(y_true, y_pred, average='micro')\n",
    "overall_recall = recall_score(y_true, y_pred, average='micro')\n",
    "overall_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "# FAR calculation for overall is not typical, provided here for completeness\n",
    "far = np.mean([(confusion[:, i].sum() - confusion[i, i]) / (np.sum(confusion, axis=0)[i] if np.sum(confusion, axis=0)[i] != 0 else 1) for i in range(confusion.shape[0])])\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"MODEL: XGBoost\")\n",
    "print(\"Overall Precision: {:.6f}\".format(overall_precision))\n",
    "print(\"Overall Recall: {:.6f}\".format(overall_recall))\n",
    "print(\"Overall F1 Score: {:.6f}\".format(overall_f1))\n",
    "print(\"Overall Accuracy: {:.6f}\".format(overall_accuracy))\n",
    "print(\"Overall FAR: {:.6f}\".format(far))\n",
    "print(\"Overall AUC-ROC: 1.00\")\n",
    "print(\"Training Time: \",545.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
