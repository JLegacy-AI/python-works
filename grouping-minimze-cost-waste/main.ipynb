{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stock_file = open('D:\\\\Python\\\\python-works\\\\grouping-minimze-cost-waste\\\\stocks.json')\n",
    "stock = json.load(stock_file)\n",
    "\n",
    "data = open('D:\\\\Python\\\\python-works\\\\grouping-minimze-cost-waste\\\\openai_response.json')\n",
    "data = json.load(data)\n",
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cmp_to_key\n",
    "import pdfplumber\n",
    "import io\n",
    "import traceback\n",
    "import openpyxl\n",
    "import json\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from fastapi import UploadFile, HTTPException\n",
    "from openai import OpenAI\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from openpyxl import load_workbook, Workbook, styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Python\\python-works\\grouping-minimze-cost-waste\\templates\n"
     ]
    }
   ],
   "source": [
    "import pdfkit\n",
    "from bidi import algorithm as bidi_algorithm\n",
    "template_dir = os.path.join(os.path.dirname(\"D:\\\\Python\\\\python-works\\\\grouping-minimze-cost-waste\\\\\"), 'templates')\n",
    "print(template_dir)\n",
    "env = Environment(loader=FileSystemLoader(template_dir))\n",
    "\n",
    "import re\n",
    "\n",
    "def is_hebrew(text):\n",
    "    # Check if the text contains Hebrew characters\n",
    "    hebrew_pattern = re.compile(r'[\\u0590-\\u05FF]')\n",
    "    return bool(hebrew_pattern.search(text))\n",
    "\n",
    "def save_results_to_dataframe(results, bars, projectName):\n",
    "    now = datetime.now()\n",
    "    today_date = now.date()\n",
    "\n",
    "    # Overview Data\n",
    "    total_cuts = len(results)\n",
    "    total_profiles = len(set([bar['profile'] for bar in bars]))\n",
    "    total_length_m = sum([bar['stock_length']\n",
    "                         for bar in bars]) / 1000  # Convert mm to m\n",
    "    total_waste_m = sum([bar['remaining_waste']\n",
    "                        for bar in bars]) / 1000  # Convert mm to m\n",
    "    \n",
    "    overview_template = env.get_template('overview_template.html.j2')\n",
    "    print(is_hebrew(projectName))\n",
    "    overview_template_html = overview_template.render({\n",
    "        \"projectName\": \"\" if is_hebrew(projectName) else projectName,\n",
    "        \"dateGenerated\":today_date.strftime(\"%d/%m/%Y\"),\n",
    "        \"totalProfiles\":total_profiles,\n",
    "        \"totalLengthM\": total_length_m,\n",
    "        \"totalWasteM\":total_waste_m,\n",
    "        \"totalCuts\":total_cuts,\n",
    "        \"isHebrew\": is_hebrew(projectName)\n",
    "    })\n",
    "\n",
    "    overview_data = [\n",
    "        [\"Overview and Summary\", \"\"],\n",
    "        [\"Project Name\", projectName],\n",
    "        [\"Date Generated\", today_date],\n",
    "        [\"Total Profiles Type\", total_profiles],\n",
    "        [\"Total Length (m)\", total_length_m],\n",
    "        [\"Total Waste (m)\", total_waste_m],\n",
    "        [\"Total Cuts\", total_cuts],\n",
    "    ]\n",
    "    overview_df = pd.DataFrame(overview_data, columns=[\"\", \"\"])\n",
    "\n",
    "    material_agg = defaultdict(\n",
    "        lambda: {'quantity': 0, 'total_length_m': 0.0, 'total_waste_mm': 0})\n",
    "\n",
    "    for bar in bars:\n",
    "        key = (bar['profile'], bar['stock_length'])\n",
    "        material_agg[key]['quantity'] += 1\n",
    "        # Convert mm to m\n",
    "        material_agg[key]['total_length_m'] += bar['stock_length'] / 1000.0\n",
    "        material_agg[key]['total_waste_mm'] += bar['remaining_waste']\n",
    "\n",
    "\n",
    "    # Material Data\n",
    "    material_data = []\n",
    "    for (profile, stock_length), data in material_agg.items():\n",
    "        # Calculate estimated waste percentage\n",
    "        total_stock_length_mm = stock_length * data['quantity']\n",
    "        waste_percentage = (\n",
    "            data['total_waste_mm'] / total_stock_length_mm) * 100 if total_stock_length_mm else 0\n",
    "        material_data.append([\n",
    "            profile,\n",
    "            stock_length,\n",
    "            data['quantity'],\n",
    "            round(data['total_length_m'], 2),\n",
    "            round(waste_percentage, 2)\n",
    "        ])\n",
    "\n",
    "    cutlist_template = env.get_template('cutlist_template.html.j2')\n",
    "    cutlist_template_html = cutlist_template.render(data = material_data)\n",
    "\n",
    "    material_df = pd.DataFrame(material_data, columns=[\n",
    "        \"Profile Type\", \"Stock Length (mm)\", \"Quantity\",\n",
    "        \"Total Length (m)\", \"Est Waste (%)\"\n",
    "    ])\n",
    "\n",
    "    stock_profile_template_data = []\n",
    "\n",
    "    # Cut List Data\n",
    "    cut_data = []\n",
    "    for bar in bars:\n",
    "        entity = {\n",
    "            \"stockProfilePOS\": bar['id'],\n",
    "            \"profileType\": bar['profile'],\n",
    "            \"stockLength\": bar['stock_length'],\n",
    "            \"wasteMM\": bar['remaining_waste'],\n",
    "            \"cuts\": []\n",
    "        }\n",
    "        # Empty row for separation\n",
    "        cut_data.append([\"Stock profile POS\", bar['id']])\n",
    "        # Empty row for separation\n",
    "        cut_data.append([\"Profile Type\", bar['profile']])\n",
    "        # Empty row for separation\n",
    "        cut_data.append([\"Stock Length\", bar['stock_length']])\n",
    "        cut_data.append([\"Waste (mm)\", bar['remaining_waste']])\n",
    "\n",
    "        cut_data.append([\"Pos Number\", \"Cut Lengths (mm)\", \"Quantity\"])\n",
    "\n",
    "\n",
    "        # Group cuts by 'posNumber' and 'cut_length'\n",
    "        cut_counts = defaultdict(int)\n",
    "        for cut in bar['cuts']:\n",
    "            key = (cut['posNumber'], cut['cut_length'])\n",
    "            cut_counts[key] += 1\n",
    "\n",
    "        # Convert the grouped cuts into a list and sort for consistency\n",
    "        grouped_cuts = [(posNumber, cut_length, quantity)\n",
    "                        for (posNumber, cut_length), quantity in cut_counts.items()]\n",
    "        # Sort by Pos Number and Cut Length\n",
    "        grouped_cuts.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        # Add grouped cuts to cut_data\n",
    "        for posNumber, cut_length, quantity in grouped_cuts:\n",
    "            entity[\"cuts\"].append({\n",
    "                \"posNumber\": posNumber,\n",
    "                \"cutLength\": cut_length,\n",
    "                \"quantity\": quantity\n",
    "            })\n",
    "            cut_data.append([posNumber, cut_length, quantity])\n",
    "        \n",
    "        stock_profile_template_data.append(entity)\n",
    "        cut_data.append([])  # Empty row for separation\n",
    "\n",
    "    stock_profile_template = env.get_template('stock_profile_template.html.j2')\n",
    "    stock_profile_html = stock_profile_template.render(data=stock_profile_template_data)\n",
    "\n",
    "    cut_list_df = pd.DataFrame(cut_data)\n",
    "\n",
    "    # Write to Excel\n",
    "    try:\n",
    "        with pd.ExcelWriter(\"Combined_Output.xlsx\", engine=\"openpyxl\") as writer:\n",
    "            overview_df.to_excel(\n",
    "                writer, sheet_name=\"Overview\", index=False, header=False)\n",
    "            material_df.to_excel(\n",
    "                writer, sheet_name=\"Material List\", index=False)\n",
    "            cut_list_df.to_excel(\n",
    "                writer, sheet_name=\"Cut List\", index=False, header=False)\n",
    "\n",
    "        # Merge Sheets\n",
    "        pdf = merge_sheets_into_one(\"Combined_Output.xlsx\", [\n",
    "                              \"Overview\", \"Material List\", \"Cut List\"], overview_template_html, cutlist_template_html, projectName ,stock_profile_html)\n",
    "        return pdf\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def merge_sheets_into_one(file_path, sheet_names, overview_html, cutlist_html, stock_profile_html ,projectName, output_sheet_name=\"MergedSheet\"):\n",
    "    # Load the workbook and select sheets to merge\n",
    "    original_wb = load_workbook(file_path)\n",
    "    new_wb = Workbook()\n",
    "    merged_sheet = new_wb.active\n",
    "    merged_sheet.title = \"Data\"  # Name for the merged sheet in the new workbook\n",
    "\n",
    "    # Row counter to keep track of where to paste data in the merged sheet\n",
    "    current_row = 1\n",
    "    first_blank_row_encountered = False\n",
    "    # Flag to ensure we start writing data only after valid rows begin\n",
    "    has_written_data = False\n",
    "    first_row_written = False  # Flag to indicate if the first row has been written\n",
    "    # Border style\n",
    "    bottom_border = styles.Border(bottom=styles.Side(\n",
    "        border_style='thin', color='000000'))\n",
    "\n",
    "    # Loop through each sheet specified in sheet_names\n",
    "    for sheet_name in sheet_names:\n",
    "        sheet = original_wb[sheet_name]\n",
    "        if sheet_name == \"Material List\":\n",
    "            merged_sheet.cell(row=current_row, column=1,\n",
    "                              value=\"Material and Cut List\")\n",
    "            current_row += 2  # Move to the next row after adding the bold text\n",
    "\n",
    "        elif sheet_name == \"Cut List\":\n",
    "            current_row += 1\n",
    "            merged_sheet.cell(row=current_row, column=1, value=\"Cut List and Optimization\").font = styles.Font(\n",
    "                bold=True, size=12)\n",
    "            current_row += 2\n",
    "\n",
    "        # Copy each row from the current sheet to the merged sheet\n",
    "        for row in sheet.iter_rows(values_only=True):\n",
    "            if not has_written_data and all(cell is None for cell in row):\n",
    "                continue  # Skip to the next row\n",
    "            has_written_data = True\n",
    "\n",
    "            # Check if the current row is blank\n",
    "            if all(cell is None for cell in row):\n",
    "                # Leave the first encountered blank row empty and continue\n",
    "                if not first_blank_row_encountered:\n",
    "                    first_blank_row_encountered = True\n",
    "                    current_row += 1  # Move to the next row in the merged sheet for insertion\n",
    "                    continue\n",
    "\n",
    "            # Copy each row from the current sheet to the merged sheet\n",
    "            for col_num, cell_value in enumerate(row, start=1):\n",
    "                cell = merged_sheet.cell(\n",
    "                    row=current_row, column=col_num, value=cell_value)\n",
    "            # Check if the cell in column A (first cell in the row) contains 'Waste'\n",
    "            cell_in_column_a = row[0] if len(row) > 0 else None\n",
    "            if cell_in_column_a and 'Waste' in str(cell_in_column_a):\n",
    "                current_row += 1  # Insert a blank row by incrementing current_row\n",
    "            current_row += 1  # Move to the next row in the merged sheet\n",
    "            if not first_row_written:\n",
    "                first_row_written = True\n",
    "                current_row += 1\n",
    "\n",
    "        current_row += 1\n",
    "\n",
    "    # Apply bold formatting to the first column in the merged sheet\n",
    "    for row in merged_sheet.iter_rows(min_row=1, max_row=current_row - 1, min_col=1, max_col=1):\n",
    "        for cell in row:\n",
    "            cell.font = styles.Font(bold=True)\n",
    "\n",
    "    # Apply bold formatting to row 9 in the merged sheet\n",
    "    for cell in merged_sheet[12]:  # Access all cells in row 9\n",
    "        cell.font = styles.Font(bold=True)\n",
    "\n",
    "    for row in range(1, current_row):  # Start from row 9 to the last row\n",
    "        cell = merged_sheet.cell(row=row, column=2)  # Column 2 is \"B\"\n",
    "        cell.alignment = styles.Alignment(horizontal=\"left\")\n",
    "    merged_sheet['A1'].font = styles.Font(bold=True, size=12)\n",
    "    merged_sheet['A10'].font = styles.Font(bold=True, size=12)\n",
    "\n",
    "    # Apply border and reset font starting from row 12 until the next blank row after row 12\n",
    "    for row in range(13, current_row):\n",
    "        # Check if the row is empty, if it is, stop applying styles\n",
    "        if all(cell.value is None for cell in merged_sheet[row]):\n",
    "            current_row += 1  # Move to the next row after the empty row\n",
    "            break  # Stop processing further rows if a blank row is encountered\n",
    "\n",
    "        for col_num in range(1, merged_sheet.max_column + 1):\n",
    "            cell = merged_sheet.cell(row=row, column=col_num)\n",
    "            # Apply the bottom border style to the cell\n",
    "            cell.border = bottom_border\n",
    "            # Reset the font to simple (non-bold)\n",
    "            cell.font = styles.Font(bold=False)\n",
    "    row_cell = merged_sheet.cell(row=1, column=1)\n",
    "    # Iterate over cells in column A and make \"Cut List and Optimization\" bold with size 12\n",
    "    row_tracker = 0\n",
    "    for row in merged_sheet.iter_rows(min_row=1, max_row=current_row, min_col=1, max_col=1):\n",
    "        for cell in row:\n",
    "            if cell.value == \"Cut List and Optimization\":\n",
    "                cell.font = styles.Font(bold=True, size=12)\n",
    "                current_row += 1\n",
    "                row_tracker = cell.row\n",
    "\n",
    "            if cell.value == \"Pos Number\":\n",
    "                for col_num in range(1, merged_sheet.max_column + 1):\n",
    "                    row_cell = merged_sheet.cell(row=cell.row, column=col_num)\n",
    "                    row_cell.font = styles.Font(bold=True)\n",
    "            # Reset the font to simple (non-bold) after \"Pos Number\" row until the next \"Pos Number\" is detected\n",
    "            if cell.value == \"Pos Number\":\n",
    "                pos_number_row = cell.row\n",
    "                for row in range(pos_number_row + 1, current_row):\n",
    "                    next_cell = merged_sheet.cell(row=row, column=1)\n",
    "                    if next_cell.value == \"Stock profile POS\":\n",
    "                        break\n",
    "                    for col_num in range(1, merged_sheet.max_column + 1):\n",
    "                        row_cell = merged_sheet.cell(row=row, column=col_num)\n",
    "                        row_cell.font = styles.Font(bold=False)\n",
    "\n",
    "    # Apply bottom border to all cells with values after row_tracker\n",
    "    for row in range(row_tracker + 1, current_row):\n",
    "        for col_num in range(1, merged_sheet.max_column + 1):\n",
    "            cell = merged_sheet.cell(row=row, column=col_num)\n",
    "            if cell.value is not None:\n",
    "                cell.border = bottom_border\n",
    "\n",
    "    # Save the workbook (you can choose to overwrite or save as a new file)\n",
    "    pdf = save_to_pdf(overview_html, cutlist_html, stock_profile_html, projectName)\n",
    "    return pdf\n",
    "\n",
    "from xhtml2pdf import pisa\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from io import BytesIO\n",
    "import io\n",
    "\n",
    "def save_to_pdf(overview_html, cutlist_html, stock_profile_html, projectName):\n",
    "    # Step 1: Generate the initial PDF using xhtml2pdf\n",
    "    html_content = f\"<!DOCTYPE html><html><head><meta charset=\\\"UTF-8\\\"></head><body>{overview_html+cutlist_html+stock_profile_html}</body></html>\"\n",
    "    pdf_buffer = io.BytesIO()\n",
    "    pisa_status = pisa.CreatePDF(html_content.encode(encoding=\"utf-8\"), dest=pdf_buffer, encoding='utf-8')\n",
    "\n",
    "    # Check for errors in PDF generation\n",
    "    if pisa_status.err:\n",
    "        print(\"Error in PDF creation:\", pisa_status.err)\n",
    "        return None\n",
    "\n",
    "    pdf_buffer.seek(0)  # Rewind the buffer to the beginning for reading\n",
    "\n",
    "    # Step 2: Add Hebrew text to the first page\n",
    "    modified_pdf_buffer = BytesIO()\n",
    "    reader = PdfReader(pdf_buffer)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # Register a font that supports Hebrew (e.g., Arial)\n",
    "    pdfmetrics.registerFont(TTFont('Noto Sans Hebrew', 'templates\\\\font\\\\Calibri.ttf'))  # Ensure arial.ttf is available on your system\n",
    "\n",
    "    # Create overlay with Hebrew text\n",
    "    overlay_buffer = BytesIO()\n",
    "    can = canvas.Canvas(overlay_buffer)\n",
    "    can.setFont(\"Noto Sans Hebrew\", 8)  # Use the registered Hebrew-supported font\n",
    "    can.drawString(100, 700, \"שלום עולם\")  # Adjust position as needed\n",
    "    can.save()\n",
    "    overlay_buffer.seek(0)\n",
    "\n",
    "    # Read the overlay PDF\n",
    "    overlay_reader = PdfReader(overlay_buffer)\n",
    "\n",
    "    # Merge the overlay onto the first page\n",
    "    first_page = reader.pages[0]\n",
    "    first_page.merge_page(overlay_reader.pages[0])\n",
    "    writer.add_page(first_page)\n",
    "\n",
    "    # Add the remaining pages\n",
    "    for page in reader.pages[1:]:\n",
    "        writer.add_page(page)\n",
    "\n",
    "    # Write the modified PDF to the new buffer\n",
    "    writer.write(modified_pdf_buffer)\n",
    "    modified_pdf_buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "    return modified_pdf_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def calculate_cuts(df, stock_lengths):\n",
    "    \"\"\"Calculate cuts based on stock availability, track waste per profile, and track bars used.\"\"\"\n",
    "    # Convert the stock lengths to mm (from meters)\n",
    "    stock_lengths = {key: [float(length) * 1000 for length in lengths]\n",
    "                     for key, lengths in stock_lengths.items()}\n",
    "\n",
    "    results = []\n",
    "    cuts = []\n",
    "    bars = []\n",
    "\n",
    "    # Generate the cuts list (profile, length, posNumber) for each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        posNumber = row['posNumber']\n",
    "        profile = row['profile']\n",
    "        quantity = int(row['quantity'])\n",
    "        length = row['length']\n",
    "        if isinstance(length, str):\n",
    "            length = float(length.split(\" \")[0])\n",
    "        else:\n",
    "            length = float(length)\n",
    "        \n",
    "        # Add the cuts for each profile\n",
    "        cuts.extend([(profile, length, posNumber) for _ in range(quantity)])\n",
    "\n",
    "    # Group the cuts by profile type\n",
    "    grouped_cuts = {}\n",
    "    for profile, cut_group in groupby(sorted(cuts, key=lambda x: x[0]), key=lambda x: x[0]):\n",
    "        grouped_cuts[profile] = list(cut_group)\n",
    "\n",
    "    # Now we'll pack cuts for each profile into the available bars\n",
    "    packed_bars = {}\n",
    "\n",
    "    for profile, cuts_list in grouped_cuts.items():\n",
    "        available_bars = stock_lengths.get(profile, [])\n",
    "        cuts_list_sorted = sorted(cuts_list, key=lambda x: x[1], reverse=True)  # Sort cuts by length, descending\n",
    "        packed_bars[profile] = []\n",
    "\n",
    "        # Sort the bars in descending order (prefer larger bars first)\n",
    "        available_bars_sorted = sorted(available_bars, reverse=True)\n",
    "\n",
    "        remaining_cuts = cuts_list_sorted[:]\n",
    "\n",
    "        # Try to fit cuts into bars with minimum waste\n",
    "        while remaining_cuts:\n",
    "            for bar_length in available_bars_sorted:\n",
    "                bar = []\n",
    "                bar_used = 0  # Keeps track of the current bar usage\n",
    "\n",
    "                # Try to fit cuts into the current bar\n",
    "                for cut in remaining_cuts[:]:  # Iterating over remaining cuts\n",
    "                    cut_length = cut[1]  # Cut length is the second element in the tuple\n",
    "                    if cut_length <= bar_length - bar_used:  # Check if the cut fits in the current bar\n",
    "                        bar.append(cut)  # Add the cut to the current bar\n",
    "                        bar_used += cut_length\n",
    "                        remaining_cuts.remove(cut)  # Remove the cut from remaining cuts\n",
    "\n",
    "                # If anything was packed into the bar, add the bar to the packed_bars\n",
    "                if bar:\n",
    "                    cuts_sum = sum([x[1] for x in bar])\n",
    "                    bar_should_used = -1\n",
    "                    for b in available_bars_sorted:\n",
    "                        if b >= cuts_sum:\n",
    "                            bar_should_used = b\n",
    "                    packed_bars[profile].append({\n",
    "                        'bar_length': bar_should_used,\n",
    "                        'cuts': bar\n",
    "                    })\n",
    "\n",
    "                # If all remaining cuts are packed, stop\n",
    "                if not remaining_cuts:\n",
    "                    break\n",
    "                break\n",
    "\n",
    "    json_file_path = os.path.join(os.path.dirname(\"D:\\\\Python\\\\python-works\\\\grouping-minimze-cost-waste\\\\\"), 'grouped_cuts.json')\n",
    "    # Save the grouped_cuts to a JSON file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(packed_bars, json_file, indent=4)\n",
    "    \n",
    "    bar_counter=0\n",
    "    for profile, groups_ in packed_bars.items():\n",
    "        for group in groups_:\n",
    "            bar_to_use = group['bar_length']\n",
    "            for cut_ in group['cuts']:\n",
    "                profile = cut_[0]\n",
    "                length = cut_[1]\n",
    "                posNumber = cut_[2]\n",
    "\n",
    "                cut_info, bar_id = find_best_stock_option(\n",
    "                    profile, length, posNumber, bars, bar_to_use)\n",
    "                if cut_info:\n",
    "                    # Assign bar_id if new bar is created\n",
    "                    if cut_info['new_bar']:\n",
    "                        bar_counter += 1\n",
    "\n",
    "                    results.append({\n",
    "                        'posNumber': posNumber,\n",
    "                        'Profile': profile,\n",
    "                        'Cut Length': length,\n",
    "                        'Cut Info': cut_info['description'],\n",
    "                        'Waste After Cut': cut_info['remaining_waste'],\n",
    "                        'Stock profile POS': bar_id\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"No suitable stock length for cut {length}mm of profile {profile}\")\n",
    "\n",
    "    return results, bars\n",
    "\n",
    "\n",
    "\n",
    "def find_best_stock_option(profile, length, posNumber, bars, bar_to_use):\n",
    "    \"\"\"Determine the best stock bar to cut from, considering existing waste and tracking individual bars.\"\"\"\n",
    "   \n",
    "    # Try to find a waste piece from existing bars\n",
    "    for bar in bars:\n",
    "        if bar['profile'] == profile and bar['remaining_waste'] >= length:\n",
    "            # Cut from existing bar\n",
    "            bar['cuts'].append({'cut_length': length, 'posNumber': posNumber})\n",
    "            bar['remaining_waste'] -= length\n",
    "            return {\n",
    "                'description': f\"Cut from bar {bar['id']} (waste remaining {bar['remaining_waste']}mm)\",\n",
    "                'remaining_waste': bar['remaining_waste'],\n",
    "                'new_bar': False\n",
    "            }, bar['id']\n",
    "\n",
    "    # Create a new bar\n",
    "    bar_id = f\"SN{len(bars) + 1}\"\n",
    "    new_bar = {\n",
    "        'id': bar_id,\n",
    "        'profile': profile,\n",
    "        'stock_length': bar_to_use,\n",
    "        'cuts': [{'cut_length': length, 'posNumber': posNumber}],\n",
    "        'remaining_waste': bar_to_use - length\n",
    "    }\n",
    "    bars.append(new_bar)\n",
    "    return {\n",
    "        'description': f\"Cut from new bar {bar_id} ({bar_to_use}mm)\",\n",
    "        'remaining_waste': new_bar['remaining_waste'],\n",
    "        'new_bar': True\n",
    "    }, bar_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PDF saved successfully as Combined_Output.pdf\n"
     ]
    }
   ],
   "source": [
    "projectName = \"זהו טקסט בעברית שיומר ל\"\n",
    "results, bars = calculate_cuts(df, stock)\n",
    "pdfBuffer =save_results_to_dataframe(results,bars,projectName)\n",
    "\n",
    "import io\n",
    "\n",
    "output_filename = \"Combined_Output.pdf\"\n",
    "with open(output_filename, 'wb') as f:\n",
    "    f.write(pdfBuffer.getvalue())  # Write the binary content of the buffer to the file\n",
    "    print(f\"PDF saved successfully as {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RHS80*3.6' 'RHS60*5' 'RHS120*80*5' 'IPE240' 'HEA220' 'UPN300' 'RHS120*5'\n",
      " 'HEA260' 'RHS60*120*3.6' 'RHS250*150*6.3' 'RHS40*3.2' 'L70*7' 'RHS50*5'\n",
      " 'RHS150*6.3' 'RHS200*100*6.3' 'RHS160*80*5' 'RHS80*120*5' 'RHS100*5'\n",
      " 'NUT_M24']\n",
      "RHS80*3.6 [6, 6, 3, 3, 2, 6]\n",
      "RHS60*5 [6, 4, 3]\n",
      "RHS120*80*5 [3, 5, 3, 2, 3]\n",
      "IPE240 [5, 3, 3, 4]\n",
      "HEA220 [3, 2, 5, 6]\n",
      "UPN300 [4, 3, 6, 5, 2]\n",
      "RHS120*5 [3, 5, 6, 4]\n",
      "HEA260 [4, 6]\n",
      "RHS60*120*3.6 [3, 4, 5]\n",
      "RHS250*150*6.3 [5, 5, 4, 5]\n",
      "RHS40*3.2 [5, 3, 2, 6, 4, 4]\n",
      "L70*7 [4, 6, 3, 3]\n",
      "RHS50*5 [6, 6, 4, 3, 5, 3]\n",
      "RHS150*6.3 [2, 5]\n",
      "RHS200*100*6.3 [2, 4, 3, 3, 5, 6]\n",
      "RHS160*80*5 [3, 5, 6, 2]\n",
      "RHS80*120*5 [4, 6, 6, 3]\n",
      "RHS100*5 [3, 4, 5, 2, 6, 2]\n",
      "NUT_M24 [6, 2, 4, 2, 4, 4]\n",
      "{'profile': 'RHS120*80*5', 'length': np.float64(5710.0), 'must_be_greater_or_equal': 5000}\n",
      "{'profile': 'RHS150*6.3', 'length': np.float64(5390.0), 'must_be_greater_or_equal': 5000}\n",
      "{'profile': 'RHS250*150*6.3', 'length': np.float64(5386.0), 'must_be_greater_or_equal': 5000}\n",
      "{'profile': 'RHS60*120*3.6', 'length': np.float64(5650.0), 'must_be_greater_or_equal': 5000}\n",
      "{'profile': 'RHS80*3.6', 'length': np.float64(6541.0), 'must_be_greater_or_equal': 6000}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = open('D:\\\\Python\\\\python-works\\\\grouping-minimze-cost-waste\\\\openai_response.json')\n",
    "data = json.load(data)\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# FIND UNIQUE PROFILES\n",
    "unique_profiles = df['profile'].unique()\n",
    "print(unique_profiles)\n",
    "\n",
    "stocks_lengths = {}\n",
    "\n",
    "for profile in unique_profiles:\n",
    "    # GENERATE RANDOM NUMBERS 2 - 6 FOR EACH PROFILE\n",
    "\n",
    "    for i in range(random.randint(2, 6)):\n",
    "        # GENERATE RANDOM NUMBERS 2 - 6 FOR EACH PROFILE\n",
    "        random_number = random.randint(2, 6)\n",
    "        stocks_lengths[profile] = stocks_lengths.get(profile, []) + [random_number]\n",
    "\n",
    "\n",
    "max_stocks_length = {}\n",
    "for profile, lengths in stocks_lengths.items():\n",
    "    max_stocks_length[profile] = max(lengths)\n",
    "    print(profile, lengths)\n",
    "\n",
    "\n",
    "# GROUP BY PROFILE\n",
    "grouped_df = df.groupby('profile')\n",
    "\n",
    "# NOW LOOP THROUGH EACH PROFILES LENGTH by Splitting\n",
    "# IT by \" \" and then take 0 Index and convert it to float\n",
    "profiles_max_length = {}\n",
    "for profile, group in grouped_df:\n",
    "    profiles_max_length[profile] = group['length'].str.split(' ').str[0].astype(float).max()\n",
    "\n",
    "\n",
    "anomalies = None\n",
    "for profile, max_length in profiles_max_length.items():\n",
    "\n",
    "    if max_stocks_length[profile]*1000 < max_length:\n",
    "\n",
    "        if anomalies is None:\n",
    "            anomalies = []\n",
    "\n",
    "        anomalies.append({\n",
    "            \"profile\": profile,\n",
    "            \"length\": max_length/1000,\n",
    "        })\n",
    "\n",
    "for anomaly in anomalies:\n",
    "    print(anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steel_profile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
